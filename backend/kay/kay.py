import requests
from langgraph import (
    Graph,
    Node,
    Result,
    Tool,
)  # Ensure this is accessible or refactor accordingly
from langchain_ai21 import AI21LLM, AI21ContextualAnswers
from langchain_core.prompts import PromptTemplate
from langchain.agents import (
    Tool as LangChainTool,
    AgentExecutor,
    LLMSingleActionAgent,
    AgentOutputParser,
)
from langchain.schema import AgentAction, AgentFinish
from langchain.vectorstores import FAISS
from langchain.embeddings import (
    OpenAIEmbeddings,
)  # Replace with JAMBA-compatible embeddings if available
from langchain.prompts import StringPromptTemplate
from langchain.memory import ConversationBufferMemory
import json
import logging
import re
import ast
import pickle
import matplotlib.pyplot as plt
import pandas as pd
import io
import base64

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Configuration
JAMBA_API_KEY = "your-jamba-api-key"
BASE_URL = "https://api.example.com"  # Make this configurable as needed

# --- Vector Store Setup ---


def load_vector_store(tools_docs):
    try:
        with open("tools_vector_store.pkl", "rb") as f:
            vector_store = pickle.load(f)
            logger.info("Loaded existing vector store.")
    except FileNotFoundError:
        # Initialize embeddings
        embeddings = OpenAIEmbeddings(
            api_key="your-openai-api-key"
        )  # Update if using JAMBA's embeddings
        texts = [doc for _, doc in tools_docs]
        vector_store = FAISS.from_texts(texts, embeddings)
        with open("tools_vector_store.pkl", "wb") as f:
            pickle.dump(vector_store, f)
            logger.info("Created and saved new vector store.")
    return vector_store


# Example list of tools and their documentation
tools_docs = [
    (
        "get_data_tool",
        "## get_data_tool\n**Endpoint:** `/getData`\n**Method:** `GET`\n**Description:** Retrieves data based on the query and limit.\n\n### Usage\n```python\nresponse = get_data_tool(params={'query': 'example', 'limit': 10})\nprint(response)\n```",
    ),
    (
        "submit_data_tool",
        "## submit_data_tool\n**Endpoint:** `/submitData`\n**Method:** `POST`\n**Description:** Submits data to the server.\n\n### Usage\n```python\nresponse = submit_data_tool(params={'data': {'key': 'value'}})\nprint(response)\n```",
    ),
    # Add more tools as needed
]

vector_store = load_vector_store(tools_docs)

# --- Language Model Integration ---


class KayOutputParser(AgentOutputParser):
    def parse(self, text):
        # Regex to capture the tool name and arguments
        tool_regex = r"Action\s*\d+:\s*([a-zA-Z_]+)\((.*)\)"
        match = re.search(tool_regex, text)
        if match:
            tool_name = match.group(1)
            args = match.group(2)
            try:
                parsed_args = ast.literal_eval(f"{{{args}}}")
            except:
                parsed_args = {}
            return AgentAction(tool=tool_name, tool_input=parsed_args, log=text)
        if "Final Answer:" in text:
            final_answer = text.split("Final Answer:")[-1].strip()
            return AgentFinish(return_values={"output": final_answer}, log=text)
        raise ValueError("Could not parse the output")


class LanguageModel:
    """Abstracts interactions with the JAMBA language model using langchain_ai21."""

    def __init__(self, api_key: str, model_name: str = "j2-ultra"):
        self.api_key = api_key
        self.model_name = model_name
        self.llm = AI21LLM(model=model_name, api_key=self.api_key)
        # Define a prompt template for generating request parameters
        self.prompt_template = PromptTemplate(
            template="""Question: {question}

Answer: Let's think step by step.""",
            input_variables=["question"],
        )
        # Initialize AI21ContextualAnswers if needed
        self.contextual_answers = AI21ContextualAnswers(api_key=self.api_key)

    def generate_completion(self, prompt: str) -> str:
        """Generates a completion using JAMBA's API via langchain_ai21."""
        try:
            # Create a chain by combining the prompt template and the LLM
            question = prompt
            chain = self.prompt_template | self.llm
            response = chain.invoke({"question": question})
            logger.info(f"Generated completion: {response}")
            return response
        except Exception as e:
            logger.error(f"Error generating completion: {e}")
            return f"Error: {str(e)}"


# --- Tool Creation ---

# Assuming tools_module.py contains the tool functions generated by asi_repl.py
import kay.tool_module as tool_module  # Ensure this module exists and contains the necessary functions


def create_tools(vector_store, tools_docs):
    tools = []
    for tool_name, doc in tools_docs:
        # Extract usage example from documentation
        usage_regex = r"```python\n(.+?)```"
        match = re.search(usage_regex, doc, re.DOTALL)
        if match:
            usage = match.group(1).strip()
            # Extract function name and parameters
            func_match = re.match(r"response\s*=\s*([a-zA-Z_]+)\((.*)\)", usage)
            if func_match:
                func_name = func_match.group(1)
                params = func_match.group(2)

                # Define the tool function
                def tool_func(params, func_name=func_name):
                    func = getattr(tool_module, func_name, None)
                    if not func:
                        return f"Tool {func_name} not found."
                    return func(params)

                # Create a LangChain Tool
                tool = LangChainTool(
                    name=tool_name,
                    func=tool_func,
                    description=doc,  # Use the documentation as the description
                )
                tools.append(tool)
                logger.info(f"Created tool: {tool_name}")
    return tools


tools = create_tools(vector_store, tools_docs)

# --- Agent Setup ---

template = """
You are Kay, an intelligent assistant that uses a set of tools to accomplish tasks. You have access to the following tools:

{tools}

Use chain-of-thought reasoning to decide which tools to use and in what order to accomplish the user's request.

Begin!

User: {input}
Thought:
"""

prompt = StringPromptTemplate(template=template, input_variables=["input", "tools"])

# Initialize the LLM
llm = AI21LLM(model="j2-ultra", api_key=JAMBA_API_KEY)

# Initialize the output parser
output_parser = KayOutputParser()

# Create the agent
agent = LLMSingleActionAgent(
    llm=llm,
    output_parser=output_parser,
    stop=["\nObservation:"],
    allowed_tools=[tool.name for tool in tools],
)

# Initialize memory
memory = ConversationBufferMemory(memory_key="chat_history")

# Create the AgentExecutor
agent_executor = AgentExecutor.from_agent_and_tools(
    agent=agent, tools=tools, verbose=True, memory=memory
)

# --- Visualization Function ---


def visualize_response(response):
    """
    Generates a visualization from the response if applicable.
    Returns the image as a base64 string.
    """
    if isinstance(response, dict):
        # Example: Create a bar chart if the response contains data
        if "data" in response and isinstance(response["data"], dict):
            df = pd.DataFrame(list(response["data"].items()), columns=["Key", "Value"])
            plt.figure(figsize=(8, 6))
            df.plot(kind="bar", x="Key", y="Value")
            plt.title("Data Visualization")
            plt.tight_layout()
            buf = io.BytesIO()
            plt.savefig(buf, format="png")
            buf.seek(0)
            img_base64 = base64.b64encode(buf.read()).decode("utf-8")
            plt.close()
            return img_base64
    return None


def present_response(response):
    """
    Presents the response in a user-friendly manner, including visualizations if available.
    """
    if isinstance(response, dict):
        # Convert the response to a readable format
        text_output = json.dumps(response, indent=4)

        # Generate visualization
        img_base64 = visualize_response(response)

        if img_base64:
            # Display the image in an HTML-friendly format
            html_image = f"<img src='data:image/png;base64,{img_base64}'/>"
            return text_output + "\n\n" + html_image
        else:
            return text_output
    else:
        return str(response)


# --- Running the Agent ---


def run_kay(user_input, agent_executor, vector_store):
    """
    Runs the Kay agent with the given user input.
    """
    # Retrieve relevant tools from the vector store based on user input
    relevant_docs = vector_store.similarity_search(
        user_input, k=3
    )  # Adjust 'k' as needed
    relevant_tools = [
        doc for doc in relevant_docs
    ]  # Assuming tools_docs is a list of tuples

    # Format the tools for the prompt
    tools_description = "\n".join([doc[1] for doc in relevant_tools])

    # Update the prompt with relevant tools
    formatted_prompt = prompt.format(tools=tools_description, input=user_input)

    # Invoke the agent
    response = agent_executor.run(user_input)

    # Present the response
    presentation = present_response(response)

    return presentation


# --- Example Usage ---

if __name__ == "__main__":
    user_input = "I need to retrieve the latest sales data and submit a summary report."

    # Run the agent
    output = run_kay(user_input, agent_executor, vector_store)

    # Display the output
    print("### Kay's Response ###")
    print(output)
